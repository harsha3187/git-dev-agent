{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction to Semantic Kernel\n",
    "\n",
    "Semantic Kernel is an open-source framework designed to seamlessly integrate AI models into applications. It provides a unified approach to managing AI capabilities, enabling developers to build intelligent, context-aware systems efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1 Key Concepts\n",
    "- **Kernel**: The core component that orchestrates AI services, plugins, and agents.\n",
    "- **Plugins**: Modular units that encapsulate specific functionalities, such as interacting with APIs or performing computations.\n",
    "- **Agents**: AI-driven entities that use the kernel to process inputs, maintain context, and generate responses.\n",
    "- **Chat Completion**: A feature that allows conversational interactions with AI models, maintaining context across exchanges.\n",
    "- **Planner**: A mechanism to break down complex tasks into smaller, manageable steps that can be executed sequentially or in parallel.\n",
    "- **Filters**: Tools to refine and customize the output of AI models based on specific criteria or constraints.\n",
    "- **Vector Store (Memory) Connectors**: Integrations that allow the kernel to store and retrieve embeddings for long-term memory, enabling context retention across sessions.\n",
    "- **Prompt Templates**: Predefined templates that structure the input prompts for AI models, ensuring consistency and efficiency in generating responses.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Components\n",
    "1. **AI Services**: Integrates with OpenAI, Azure OpenAI, and other AI providers for tasks like text generation and summarization.\n",
    "2. **Plugins**: Extensible modules for tasks like GitHub integration, data analysis, or custom workflows.\n",
    "3. **Execution Settings**: Configurations for managing AI model behavior, such as token limits and response formats.\n",
    "4. **Chat History**: Maintains a record of interactions to provide context-aware responses.\n",
    "5. **Planner**: Enables dynamic task planning and execution by breaking down user requests into actionable steps.\n",
    "6. **Vector Store (Memory)**: Stores embeddings for long-term memory, allowing the kernel to recall past interactions or knowledge.\n",
    "7. **Prompt Templates**: Simplifies the creation of structured prompts for consistent and effective communication with AI models.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Real-World Examples\n",
    "- **Customer Support**: Automating responses to common queries while escalating complex issues to human agents.\n",
    "- **Code Review**: Analyzing pull requests and suggesting improvements using GitHub plugins.\n",
    "- **Content Creation**: Generating blog posts, summaries, or reports with minimal input.\n",
    "- **Data Analysis**: Extracting insights from datasets and presenting them in a user-friendly format.\n",
    "- **Task Automation**: Using planners to decompose and execute complex workflows.\n",
    "- **Contextual Search**: Leveraging vector stores to retrieve relevant information based on embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "Semantic Kernel empowers developers to create intelligent, modular, and extensible applications by combining the power of AI with a unified framework. Its support for planners, filters, vector stores, and prompt templates makes it a versatile tool for building advanced AI-driven solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Semantic Kernel Git Chat Agent\n",
    "\n",
    "The Semantic Kernel Git Chat Agent is designed to facilitate chat-based interactions with an AI model, leveraging the power of Semantic Kernel's unified framework. This framework integrates the chat-completion capabilities of various AI models, enabling seamless communication.\n",
    "\n",
    "The Git Chat Agent maintains a chat history that is presented to the AI model with each request, ensuring context-aware responses. It can generate responses directed to users or interact with other agents, making it a versatile tool for managing GitHub repositories and related tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prerequisites\n",
    "\n",
    "Before proceeding with the steps in this notebook, ensure the following prerequisites are met:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Generate a GitHub Personal Access Token (PAT)\n",
    "A Personal Access Token (PAT) is required to interact with the GitHub API. Follow these steps to generate a PAT:\n",
    "\n",
    "1. Log in to your GitHub account.\n",
    "2. Navigate to **Settings** > **Developer settings** > **Personal access tokens** > **Tokens (classic)**.\n",
    "3. Click on **Generate new token**.\n",
    "4. Select the required scopes:\n",
    "   - **repo**: Full control of private repositories.\n",
    "   - **read:org**: Read-only access to organization membership.\n",
    "   - **user**: Read access to user profile data.\n",
    "5. Set an expiration date for the token.\n",
    "6. Click **Generate token** and copy the token. **Save it securely**, as it will not be shown again.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Add Keys to the `.env` File\n",
    "Rename the `.env.example` to `.env` file in the root directory of your project and add the following keys:\n",
    "\n",
    "```plaintext\n",
    "# OpenAI API Keys\n",
    "AZURE_OPENAI_ENDPOINT=<Your Azure OpenAI Endpoint> -- will be provided during the lab\n",
    "AZURE_OPENAI_API_KEY=<Your Azure OpenAI API Key> -- will be provided during the lab\n",
    "\n",
    "# GitHub PAT\n",
    "GITHUB_PAT=<Your GitHub Personal Access Token> -- Add your PAT from above step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the current version of Semantic Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.29.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from semantic_kernel import __version__\n",
    "\n",
    "__version__ #1.29.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load variables from .env file for local development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "if os.path.exists(\".env\"):\n",
    "    load_dotenv(override=True)\n",
    "else:\n",
    "    # Load environment variables from the parent \n",
    "    current_dir = os.getcwd()\n",
    "    env_path = os.path.join(current_dir, '..', '.env')\n",
    "    load_dotenv(dotenv_path=env_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Creating a chat completion service\n",
    "\n",
    ">Note: The AzureChatCompletion service also supports Microsoft Entra authentication. If you don't provide an API key, the service will attempt to authenticate using the Entra token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "# Add Azure OpenAI chat completion\n",
    "chat_completion = AzureChatCompletion(\n",
    "    endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    service_id=\"serv-git-chat-1\" # Used for calling the service in the kernel and settings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Let's try chatting with LLM model , if this works you are good to go for next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "\n",
    "execution_settings = AzureChatPromptExecutionSettings()\n",
    "\n",
    "system_message = \"\"\"\n",
    "You are a helpful assistant that can answer questions and provide information.\n",
    "\"\"\"\n",
    "\n",
    "chat_history = ChatHistory(system_message=system_message)\n",
    "\n",
    "chat_history.add_user_message(\"Hello, who are you , what's your model name/version , how much token length you can accept from Azure deployment?\")\n",
    "\n",
    "response = await chat_completion.get_chat_message_content(\n",
    "    chat_history=chat_history,\n",
    "    settings=execution_settings,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating a Chat Completion Agent\n",
    "\n",
    "A chat completion agent is fundamentally based on AI services. Creating a chat completion agent starts with creating a Kernel instance that contains one or more chat-completion services and then instantiating the agent with a reference to that Kernel instance.\n",
    "\n",
    "---\n",
    "\n",
    "### Difference Between Chat Completion Service and Chat Completion Agent\n",
    "\n",
    "- **Chat Completion Service**:\n",
    "  - A low-level interface that directly interacts with the AI model to process and generate responses.\n",
    "  - It requires explicit management of chat history, prompt formatting, and execution settings.\n",
    "  - Example: AzureChatCompletion is a service that connects to Azure OpenAI to handle chat-based interactions.\n",
    "\n",
    "- **Chat Completion Agent**:\n",
    "  - A higher-level abstraction built on top of chat completion services.\n",
    "  - It manages chat history, context, and execution settings automatically, simplifying the interaction process.\n",
    "  - Agents can integrate additional functionalities, such as plugins or task-specific instructions, to enhance their capabilities.\n",
    "  - Example: ChatCompletionAgent uses a Kernel instance with one or more services to provide intelligent, context-aware responses.\n",
    "\n",
    "By combining chat completion services with agents, developers can build robust, context-aware conversational systems with minimal effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "from semantic_kernel.functions.kernel_arguments import KernelArguments\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings\n",
    "\n",
    "# Define the Kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "# Add the AzureChatCompletion AI Service to the Kernel\n",
    "kernel.add_service(chat_completion)\n",
    "\n",
    "settings = AzureChatPromptExecutionSettings(service_id=\"serv-git-chat-1\")\n",
    "\n",
    "# Create the agent\n",
    "agent = ChatCompletionAgent(\n",
    "    kernel=kernel, \n",
    "    name=\"chat-agent\", \n",
    "    instructions=\"You are a helpful agent.\",\n",
    "    arguments=KernelArguments(settings=settings)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Conversing with ChatCompletionAgent\n",
    "\n",
    "- There are multiple ways to converse with a ChatCompletionAgent.\n",
    "\n",
    "- The easiest is to call and await get_response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.contents.chat_message_content import ChatMessageContent\n",
    "from semantic_kernel.contents import AuthorRole\n",
    "\n",
    "# Define the chat history\n",
    "history = ChatHistory()\n",
    "\n",
    "# Add the user message\n",
    "history.add_message(ChatMessageContent(role=AuthorRole.USER, content=\"Hello, how are you?\"))\n",
    "\n",
    "# Generate the agent response\n",
    "response = await agent.get_response(messages=history,arguments=KernelArguments(settings=settings))\n",
    "\n",
    "print(f\"{response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating a Semantic Kernel Plugin\n",
    "\n",
    "Plugins in Semantic Kernel are modular units designed to encapsulate specific functionalities, making it easier to extend the capabilities of the kernel. In this lab, we are creating a plugin to interact with the GitHub API, enabling the retrieval of repository and user information, as well as managing issues and commits.\n",
    "\n",
    "The purpose of creating this plugin is to demonstrate how Semantic Kernel can be integrated with external APIs to build intelligent, task-specific solutions. By using plugins, we can simplify complex workflows and enhance the functionality of the chat agent.\n",
    "\n",
    "This plugin example was provided by Microsoft at this link: [GitHub Plugin Example](https://github.com/microsoft/semantic-kernel/blob/main/python/samples/learn_resources/plugins/GithubPlugin/github.py).\n",
    "\n",
    "---\n",
    "\n",
    "### Alternative Approach\n",
    "\n",
    "You can also use the **MCP Server Agents** to perform similar tasks. MCP Server Agents provide a robust framework for managing and automating tasks using pre-configured agents. For more information, refer to the following resources:\n",
    "\n",
    "- [MCP Server Agents Documentation](https://docs.anthropic.com/en/docs/agents-and-tools/mcp)\n",
    "- [MCP GitHub Repository](https://github.com/modelcontextprotocol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from semantic_kernel.functions.kernel_function_decorator import kernel_function\n",
    "\n",
    "# region GitHub Models\n",
    "\n",
    "\n",
    "class Repo(BaseModel):\n",
    "    id: int = Field(..., alias=\"id\")\n",
    "    name: str = Field(..., alias=\"full_name\")\n",
    "    description: str | None = Field(default=None, alias=\"description\")\n",
    "    url: str = Field(..., alias=\"html_url\")\n",
    "\n",
    "\n",
    "class User(BaseModel):\n",
    "    id: int = Field(..., alias=\"id\")\n",
    "    login: str = Field(..., alias=\"login\")\n",
    "    name: str | None = Field(default=None, alias=\"name\")\n",
    "    company: str | None = Field(default=None, alias=\"company\")\n",
    "    url: str = Field(..., alias=\"html_url\")\n",
    "\n",
    "\n",
    "class Label(BaseModel):\n",
    "    id: int = Field(..., alias=\"id\")\n",
    "    name: str = Field(..., alias=\"name\")\n",
    "    description: str | None = Field(default=None, alias=\"description\")\n",
    "\n",
    "\n",
    "class Issue(BaseModel):\n",
    "    id: int = Field(..., alias=\"id\")\n",
    "    number: int = Field(..., alias=\"number\")\n",
    "    url: str = Field(..., alias=\"html_url\")\n",
    "    title: str = Field(..., alias=\"title\")\n",
    "    state: str = Field(..., alias=\"state\")\n",
    "    labels: list[Label] = Field(..., alias=\"labels\")\n",
    "    when_created: str | None = Field(default=None, alias=\"created_at\")\n",
    "    when_closed: str | None = Field(default=None, alias=\"closed_at\")\n",
    "\n",
    "\n",
    "class IssueDetail(Issue):\n",
    "    body: str | None = Field(default=None, alias=\"body\")\n",
    "\n",
    "\n",
    "# endregion\n",
    "\n",
    "\n",
    "class GitHubSettings(BaseModel):\n",
    "    base_url: str = \"https://api.github.com\"\n",
    "    token: str\n",
    "\n",
    "\n",
    "class GitHubPlugin:\n",
    "    def __init__(self, settings: GitHubSettings):\n",
    "        self.settings = settings\n",
    "\n",
    "    @kernel_function\n",
    "    async def get_user_profile(self) -> \"User\":\n",
    "        async with self.create_client() as client:\n",
    "            response = await self.make_request(client, \"/user\")\n",
    "            return User(**response)\n",
    "\n",
    "    @kernel_function\n",
    "    async def get_repository(self, organization: str, repo: str) -> \"Repo\":\n",
    "        async with self.create_client() as client:\n",
    "            response = await self.make_request(client, f\"/repos/{organization}/{repo}\")\n",
    "            return Repo(**response)\n",
    "\n",
    "    @kernel_function\n",
    "    async def get_issues(\n",
    "        self,\n",
    "        organization: str,\n",
    "        repo: str,\n",
    "        max_results: int | None = None,\n",
    "        state: str = \"\",\n",
    "        label: str = \"\",\n",
    "        assignee: str = \"\",\n",
    "    ) -> list[\"Issue\"]:\n",
    "        async with self.create_client() as client:\n",
    "            path = f\"/repos/{organization}/{repo}/issues?\"\n",
    "            path = self.build_query(path, \"state\", state)\n",
    "            path = self.build_query(path, \"assignee\", assignee)\n",
    "            path = self.build_query(path, \"labels\", label)\n",
    "            path = self.build_query(path, \"per_page\", str(max_results) if max_results else \"\")\n",
    "            response = await self.make_request(client, path)\n",
    "            return [Issue(**issue) for issue in response]\n",
    "    \n",
    "    @kernel_function\n",
    "    async def get_commits(\n",
    "        self,\n",
    "        organization: str,\n",
    "        repo: str,\n",
    "        max_results: int | None = None,\n",
    "        author: str = \"\",\n",
    "        since: str = \"\",\n",
    "        until: str = \"\",\n",
    "    ) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Retrieve commits from a GitHub repository.\n",
    "\n",
    "        Args:\n",
    "            organization (str): The organization or user name.\n",
    "            repo (str): The repository name.\n",
    "            max_results (int, optional): Maximum number of commits to return.\n",
    "            author (str, optional): Filter by commit author.\n",
    "            since (str, optional): Only commits after this date (ISO 8601).\n",
    "            until (str, optional): Only commits before this date (ISO 8601).\n",
    "\n",
    "        Returns:\n",
    "            list[dict]: A list of commit data dictionaries.\n",
    "        \"\"\"\n",
    "        async with self.create_client() as client:\n",
    "            path = f\"/repos/{organization}/{repo}/commits?\"\n",
    "            path = self.build_query(path, \"author\", author)\n",
    "            path = self.build_query(path, \"since\", since)\n",
    "            path = self.build_query(path, \"until\", until)\n",
    "            path = self.build_query(path, \"per_page\", str(max_results) if max_results else \"\")\n",
    "            response = await self.make_request(client, path)\n",
    "            return response\n",
    "        \n",
    "    @kernel_function\n",
    "    async def get_commit_detail(\n",
    "        self,\n",
    "        organization: str,\n",
    "        repo: str,\n",
    "        commit_sha: str,\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Retrieve details for a specific commit in a GitHub repository.\n",
    "\n",
    "        Args:\n",
    "            organization (str): The organization or user name.\n",
    "            repo (str): The repository name.\n",
    "            commit_sha (str): The commit SHA.\n",
    "\n",
    "        Returns:\n",
    "            dict: The commit details.\n",
    "        \"\"\"\n",
    "        async with self.create_client() as client:\n",
    "            path = f\"/repos/{organization}/{repo}/commits/{commit_sha}\"\n",
    "            response = await self.make_request(client, path)\n",
    "            return response\n",
    "        \n",
    "    @kernel_function\n",
    "    async def get_commit_diff(\n",
    "        self,\n",
    "        organization: str,\n",
    "        repo: str,\n",
    "        base_commit: str,\n",
    "        head_commit: str,\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Retrieve the diff (code changes) between two commits in a GitHub repository.\n",
    "\n",
    "        Args:\n",
    "            organization (str): The organization or user name.\n",
    "            repo (str): The repository name.\n",
    "            base_commit (str): The base commit SHA.\n",
    "            head_commit (str): The head commit SHA.\n",
    "\n",
    "        Returns:\n",
    "            dict: The comparison result including files changed, commits, and diff stats.\n",
    "        \"\"\"\n",
    "        async with self.create_client() as client:\n",
    "            path = f\"/repos/{organization}/{repo}/compare/{base_commit}...{head_commit}\"\n",
    "            response = await self.make_request(client, path)\n",
    "            return response\n",
    "        \n",
    "    @kernel_function\n",
    "    async def create_git_issue_with_labels(\n",
    "        self,\n",
    "        organization: str,\n",
    "        repo: str,\n",
    "        title: str,\n",
    "        body: str ,\n",
    "        labels: list[str]\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Create a new issue with dummy text and labels in the specified repository.\n",
    "\n",
    "        Args:\n",
    "            organization (str): The organization or user name.\n",
    "            repo (str): The repository name.\n",
    "            title (str, optional): The title of the issue.\n",
    "            body (str, optional): The body content of the issue.\n",
    "            labels (list[str], optional): List of labels to assign.\n",
    "\n",
    "        Returns:\n",
    "            dict: The created issue details.\n",
    "        \"\"\"\n",
    "        async with self.create_client() as client:\n",
    "            path = f\"/repos/{organization}/{repo}/issues\"\n",
    "            payload = {\"title\": title, \"body\": body, \"labels\": labels}\n",
    "            print(f\"POST REQUEST: {path}\\nPayload: {payload}\")\n",
    "            response = await client.post(path, json=payload)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "\n",
    "    @kernel_function\n",
    "    async def get_issue_detail(self, organization: str, repo: str, issue_id: int) -> \"IssueDetail\":\n",
    "        async with self.create_client() as client:\n",
    "            path = f\"/repos/{organization}/{repo}/issues/{issue_id}\"\n",
    "            response = await self.make_request(client, path)\n",
    "            return IssueDetail(**response)\n",
    "\n",
    "    def create_client(self) -> httpx.AsyncClient:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"request\",\n",
    "            \"Accept\": \"application/vnd.github+json\",\n",
    "            \"Authorization\": f\"Bearer {self.settings.token}\",\n",
    "            \"X-GitHub-Api-Version\": \"2022-11-28\",\n",
    "        }\n",
    "        return httpx.AsyncClient(base_url=self.settings.base_url, headers=headers, timeout=5)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_query(path: str, key: str, value: str) -> str:\n",
    "        if value:\n",
    "            return f\"{path}{key}={value}&\"\n",
    "        return path\n",
    "\n",
    "    @staticmethod\n",
    "    async def make_request(client: httpx.AsyncClient, path: str) -> dict:\n",
    "        print(f\"REQUEST: {path}\\n\")\n",
    "        response = await client.get(path)\n",
    "        response.raise_for_status()\n",
    "        return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Agent Definition\n",
    "\n",
    "Finally we are ready to instantiate a ChatCompletionAgent with its Instructions, associated Kernel, and the default Arguments and Execution Settings. In this case, we desire to have the any plugin functions automatically executed.\n",
    "\n",
    ">You will need to define settings for either OpenAI or Azure OpenAI and also for GitHub , before coming to this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "from semantic_kernel.contents.utils.author_role import AuthorRole\n",
    "from semantic_kernel.functions.kernel_arguments import KernelArguments\n",
    "from semantic_kernel.kernel import Kernel\n",
    "\n",
    "# Import GitHubPlugin and GitHubSettings directly from the notebook\n",
    "from __main__ import GitHubPlugin, GitHubSettings\n",
    "\n",
    "inst_template = f\"\"\"You are an agent designed to query and retrieve information from a single GitHub repository in a read-only manner.\n",
    "        You are also able to access the profile of the active user.\n",
    "        Use the current date and time to provide up-to-date details or time-sensitive responses.\n",
    "        Use the values from arguments:\n",
    "        - The repository name: {{repo_name}}\n",
    "        - The current datetime: {{now}}\n",
    "\"\"\"\n",
    "kernel = Kernel()\n",
    "# Add the AzureChatCompletion AI Service to the Kernel\n",
    "service_id = \"serv-git-chat-1\"\n",
    "kernel.add_service(AzureChatCompletion(service_id=service_id))\n",
    "\n",
    "settings = kernel.get_prompt_execution_settings_from_service_id(service_id=service_id)\n",
    "# Configure the function choice behavior to auto invoke kernel functions\n",
    "settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "# Set your GitHub Personal Access Token (PAT) value here\n",
    "gh_settings = GitHubSettings(token=os.getenv(\"GITHUB_PAT\"))\n",
    "\n",
    "kernel.add_plugin(plugin=GitHubPlugin(gh_settings), plugin_name=\"GithubPlugin\")\n",
    "\n",
    "# Create the agent\n",
    "agent = ChatCompletionAgent(\n",
    "    kernel=kernel,\n",
    "    name=\"GitAssistantAgent\",\n",
    "    instructions=inst_template,\n",
    "    arguments=KernelArguments(settings=settings,repo_name=\"harsha3187/git-dev-agent\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 All set! Now, let's test the agent with a simple query to get the user profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REQUEST: /user\n",
      "\n",
      "Your GitHub username is harsha3187.\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.contents.chat_message_content import ChatMessageContent\n",
    "from datetime import datetime\n",
    "\n",
    "chat_history = ChatHistory()\n",
    "\n",
    "user_input = \"What is my username?\"\n",
    "\n",
    "chat_history.add_message(ChatMessageContent(role=AuthorRole.USER, content=user_input))\n",
    "\n",
    "async for response in agent.invoke(messages=chat_history):\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Lets ask about the repo details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REQUEST: /repos/microsoft/semantic-kernel\n",
      "\n",
      "The repository microsoft/semantic-kernel is designed to help developers integrate cutting-edge large language model (LLM) technology quickly and easily into their applications. You can find more information and access the repository here: https://github.com/microsoft/semantic-kernel.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Describe the repo microsoft/semantic-kernel\"\n",
    "\n",
    "chat_history.add_message(ChatMessageContent(role=AuthorRole.USER, content=user_input))\n",
    "\n",
    "arguments = KernelArguments(now=datetime.now().strftime(\"%Y-%m-%d %H:%M\"))\n",
    "\n",
    "async for response in agent.invoke(messages=chat_history, arguments=arguments):\n",
    "    print(f\"{response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Describe the newest issue created in the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Describe the newest issue created in the repo.\"\n",
    "\n",
    "chat_history.add_message(ChatMessageContent(role=AuthorRole.USER, content=user_input))\n",
    "\n",
    "arguments = KernelArguments(now=datetime.now().strftime(\"%Y-%m-%d %H:%M\"))\n",
    "\n",
    "async for response in agent.invoke(messages=chat_history, arguments=arguments):\n",
    "    print(f\"{response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 List the top 10 issues closed within the last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"List the top 10 issues closed within the last week.\"\n",
    "\n",
    "chat_history.add_message(ChatMessageContent(role=AuthorRole.USER, content=user_input))\n",
    "\n",
    "arguments = KernelArguments(now=datetime.now().strftime(\"%Y-%m-%d %H:%M\"))\n",
    "\n",
    "async for response in agent.invoke(messages=chat_history, arguments=arguments):\n",
    "    print(f\"{response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And more examples..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"How were these issues labeled?\"\n",
    "\n",
    "chat_history.add_message(ChatMessageContent(role=AuthorRole.USER, content=user_input))\n",
    "\n",
    "arguments = KernelArguments(now=datetime.now().strftime(\"%Y-%m-%d %H:%M\"))\n",
    "\n",
    "async for response in agent.invoke(messages=chat_history, arguments=arguments):\n",
    "    print(f\"{response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"List the 5 most recently opened issues with the 'Bug' label.\"\n",
    "\n",
    "chat_history.add_message(ChatMessageContent(role=AuthorRole.USER, content=user_input))\n",
    "\n",
    "arguments = KernelArguments(now=datetime.now().strftime(\"%Y-%m-%d %H:%M\"))\n",
    "\n",
    "async for response in agent.invoke(messages=chat_history, arguments=arguments):\n",
    "    print(f\"{response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"What changed from Issue#412 to Issue#413?\"\n",
    "\n",
    "chat_history.add_message(ChatMessageContent(role=AuthorRole.USER, content=user_input))\n",
    "\n",
    "arguments = KernelArguments(now=datetime.now().strftime(\"%Y-%m-%d %H:%M\"))\n",
    "\n",
    "async for response in agent.invoke(messages=chat_history, arguments=arguments):\n",
    "    print(f\"{response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Give me the last 5 commits with brief descriptions?\"\n",
    "\n",
    "chat_history.add_message(ChatMessageContent(role=AuthorRole.USER, content=user_input))\n",
    "\n",
    "arguments = KernelArguments(now=datetime.now().strftime(\"%Y-%m-%d %H:%M\"))\n",
    "\n",
    "async for response in agent.invoke(messages=chat_history, arguments=arguments):\n",
    "    print(f\"{response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Additional Examples to Explore\n",
    "\n",
    "Please try the following examples and add any relevant code snippets to the plugin functions as needed:\n",
    "\n",
    "1. **Retrieve the list of contributors for a repository**  \n",
    "   - Example: Fetch all contributors for the repository `microsoft/semantic-kernel`.\n",
    "\n",
    "2. **Retrieve pull requests for a repository**  \n",
    "   - Example: List all open pull requests for the repository `microsoft/semantic-kernel`.\n",
    "\n",
    "3. **Analyze the most active contributors**  \n",
    "   - Example: Identify contributors with the highest number of commits or pull requests.\n",
    "\n",
    "4. **Who are the most active contributors in the repo microsoft/semantic-kernel?**  \n",
    "   - Example: Analyze the activity of contributors in the `microsoft/semantic-kernel` repository to determine the most active ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Learning Materials for Semantic Kernel\n",
    "\n",
    "### Official Documentation\n",
    "- [Semantic Kernel Documentation](https://learn.microsoft.com/en-us/semantic-kernel/)\n",
    "- [Semantic Kernel GitHub Repository](https://github.com/microsoft/semantic-kernel)\n",
    "\n",
    "### Tutorials and Examples\n",
    "- [Getting Started with Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/get-started/)\n",
    "- [Example Chat Agent](https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/examples/example-chat-agent?pivots=programming-language-python)\n",
    "- [Semantic Kernel Python Samples](https://github.com/microsoft/semantic-kernel/tree/main/python/samples)\n",
    "\n",
    "### Community and Support\n",
    "- [Semantic Kernel Discussions](https://github.com/microsoft/semantic-kernel/discussions)\n",
    "- [Stack Overflow - Semantic Kernel](https://stackoverflow.com/questions/tagged/semantic-kernel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic-kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
